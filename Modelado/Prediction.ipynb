{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de paquetes necesarios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import requests, json\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exportamos el dataset limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_predictive/clean_sold.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seteamos la Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En base a los experimentos realizados en la carpeta de Validaciones, vamos a generar el modelo con OverSampling con SMOTE y RAndom forest con los mejores hiperamentros encontramos durante la hiperoptimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"sold_quantity\"]\n",
    "X = df.drop([\"sold_quantity\"],axis=1)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_sm, y_sm = smote.fit_sample(X, y)\n",
    "\n",
    "X_sm_sample = X_sm.sample(frac=0.3)\n",
    "y_sm_sample = y_sm[X_sm_sample.index]\n",
    "\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_sm_sample, y_sm_sample, train_size=0.7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elijo los mejores hiperparametros encontrados en la validacion del modelo Random Forest con Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={'bootstrap': False, 'criterion': 'entropy', 'max_depth': 200, 'max_features': 0.5, 'n_estimators': 650}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=200,\n",
       "                       max_features=0.5, n_estimators=650)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**param)\n",
    "rf.fit(X_train_s,y_train_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenando el modelo con los datos oversampleados y evaluamos en las clases totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_predictive/Model_RFover'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(rf,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8702750253859802"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rf.predict(X_test_s)\n",
    "f1_score(y_test_s,predictions,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para verificar que no estamos prediciendo el mismo algoritmo que usamos para oversamplear calculamos el Score F1 para predecir la muestra de Testeo con todas muestras verdaderas. Este es el Score más representativo de nuestro modelo y supera ampliamente el valor summy de 50% y el modelo sin optimizar ni oversamplear de 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337900130191396"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,predictions,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline para nuevas publicaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo las funciones auxiliares para hacer las transforamciones necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mowing_json(dict_input): # Descartamos ramas del json que no son de interes en el analisis.\n",
    "    dict_mowing  = dict_input\n",
    "    dict_mowing.pop(\"attributes\", None)\n",
    "    dict_mowing.pop(\"pictures\", None)\n",
    "    dict_mowing.pop(\"permalink\", None)\n",
    "    dict_mowing.pop(\"variations\", None)\n",
    "    dict_mowing.pop(\"thumbnail\", None)\n",
    "    dict_mowing.pop(\"sale_terms\", None)\n",
    "    dict_mowing.pop(\"secure_thumbnail\", None)\n",
    "    dict_mowing.pop(\"video_id\", None)\n",
    "    dict_mowing.pop(\"subtitle\", None)\n",
    "    dict_mowing.pop(\"site_id\", None)\n",
    "    dict_mowing.pop(\"seller_address\", None)\n",
    "    dict_mowing.pop(\"official_store_id\", None)\n",
    "    dict_mowing.pop(\"warnings\", None)\n",
    "    dict_mowing.pop(\"seller_contact\", None)\n",
    "    dict_mowing.pop(\"international_delivery_mode\", None)\n",
    "    dict_mowing.pop(\"sub_status\", None)\n",
    "    dict_mowing.pop(\"catalog_product_id\", None)\n",
    "    dict_mowing.pop(\"parent_item_id\", None)\n",
    "    dict_mowing.pop(\"differential_pricing\", None)\n",
    "    dict_mowing.pop(\"listing_source\", None)\n",
    "    dict_mowing.pop(\"location\", None)\n",
    "    dict_mowing.pop(\"non_mercado_pago_payment_methods\", None)\n",
    "    dict_mowing.pop(\"coverage_areas\", None)\n",
    "    dict_mowing.pop(\"deal_ids\", None)\n",
    "    dict_mowing.pop(\"seller_id\", None)\n",
    "    dict_mowing.pop(\"descriptions\", None)\n",
    "    return dict_mowing\n",
    "\n",
    "def categ_sold(x):\n",
    "    if 5<= x<=50:\n",
    "        x=5\n",
    "    elif 51<= x<=100:\n",
    "        x=50\n",
    "    elif 101<= x<=150:\n",
    "        x=100\n",
    "    elif 151<= x<=200:\n",
    "        x=150\n",
    "    elif 201<= x<=250:\n",
    "        x=200\n",
    "    elif 251<= x<=500:\n",
    "        x=250    \n",
    "    elif 501<= x<=5000:\n",
    "        x=500    \n",
    "    elif 5001<= x<=50000:\n",
    "        x=5000      \n",
    "    return x\n",
    "\n",
    "\n",
    "def create_dummis(df, values,prefix=\"\"):\n",
    "    column = df[\"buying_mode\"][0]\n",
    "    for x in values:\n",
    "        if x == column:\n",
    "            df[f\"{prefix}{x}\"] = 1\n",
    "        else:\n",
    "            df[f\"{prefix}{x}\"] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion que transforma el Json de una publicacion ID y aplica el modelo y devuelve la predcción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sold(sample_id,model):\n",
    "    #Conversion Json to DataFrame\n",
    "    query = requests.get(f\"https://api.mercadolibre.com/items/{sample_id}\")\n",
    "    item = json.loads(query.content)\n",
    "    df_json = [mowing_json(x) for x in [item]] \n",
    "    df = pd.json_normalize(df_json,sep=\"_\") # Transformo Json a DataFrame\n",
    "    df = df.infer_objects()  # Inferimos el mejor tipo de dato para la columna\n",
    "    df = df.replace('', np.nan) # Reemplazo espacios vacios por NaN\n",
    "    \n",
    "    # Conversion Variables Categoricas\n",
    "    df[\"listing_type_id\"] =df[\"listing_type_id\"].map({'gold_special':6, 'gold_premium': 5, \"gold_pro\": 4,'gold':3, 'silver': 2, \"bronze\": 1,\"free\": 1})\n",
    "    buying_mode = [\"buy_it_now\",\"classified\"]\n",
    "    currency = [\"ARS\",\"USD\"]\n",
    "    log= [\"cross_docking\",\"custom\",\"default\",\"drop_off\",\"fulfillment\",\"not_specified\",\"xd_drop_off\"]\n",
    "    shippinmode = [\"custom\",\"me1\",\"me2\",\"not_specified\"]\n",
    "    status = [\"active\",\"closed\",\"inactive\",\"paused\",\"under_review\"]\n",
    "    condition = [\"new\",\"not_specified\",\"used\"]\n",
    "    create_dummis(df, buying_mode,prefix=\"bmode__\")\n",
    "    create_dummis(df, currency,prefix =\"currency__\")\n",
    "    create_dummis(df, log ,prefix=\"slog__\")\n",
    "    create_dummis(df, shippinmode,prefix=\"smd__\")\n",
    "    create_dummis(df, status)\n",
    "    create_dummis(df, condition)\n",
    "    df = df.drop(['buying_mode',\"currency_id\",\"shipping_logistic_type\",\"shipping_mode\",\"status\",\"condition\"],axis=1,errors=\"ignore\")\n",
    "    \n",
    "    # Imputacion de Variables\n",
    "    df[\"health\"].fillna(0.8,inplace=True)\n",
    "    df[\"original_price\"].fillna(0,inplace=True)\n",
    "    df[\"geolocation_latitude\"].fillna(0,inplace=True)\n",
    "    df[\"geolocation_longitude\"].fillna(0,inplace=True)\n",
    "    df = df.drop(df[df[\"available_quantity\"].isnull()].index)\n",
    "    \n",
    "    #Transformacion Features temporales\n",
    "    datetime_features = [\"start_time\",\"stop_time\",\"last_updated\",\"date_created\"]\n",
    "    call_date = datetime.datetime(2020, 8,1)\n",
    "    for feat in datetime_features:\n",
    "        df[feat] = pd.to_datetime(df[feat], utc=True).dt.tz_localize(None)\n",
    "        df[f\"{feat}_days\"] = (df[feat]- call_date).apply(lambda x : x.days)\n",
    "    df.drop(datetime_features, axis= 1 ,inplace =True, errors='ignore')\n",
    "    stopwords_es =  stopwords.words('spanish')\n",
    "    \n",
    "    #Transformacion title\n",
    "    def title2word( title):\n",
    "        return len(([x for x in title.strip().lower().split(\" \") if x not in stopwords_es]))\n",
    "    df[\"title_words\"] = df[\"title\"].apply(title2word)\n",
    "    \n",
    "    #Transformacion Tags\n",
    "    df = df.drop(df[df.duplicated(subset=['id'],keep=\"first\")].index)\n",
    "    df[\"tags\"] = df[\"tags\"].apply(len)\n",
    "    df[\"shipping_tags\"] = df[\"shipping_tags\"].apply(len)\n",
    "    \n",
    "    # Limpieza de columnas\n",
    "    df=df.drop([\"tags\",\"domain_id\",\"shipping_tags\",\"id\",\"title\",\"warranty\",\"category_id\",\"shipping_dimensions\",\"shipping_methods\",\"shipping_free_methods\"],axis=1,errors=\"ignore\")\n",
    "    df = df.drop(df[df[\"price\"].isnull()].index)\n",
    "    df[\"sold_quantity\"]=df[\"sold_quantity\"].apply(lambda x: categ_sold(x)).astype(\"int\")\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    predictions = model.predict(a.drop(\"sold_quantity\",axis=1))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([250], dtype=int64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sold(\"MLA842101865\",rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
